Mobile price minor project
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
​
import warnings
warnings.filterwarnings('ignore')
df=pd.read_csv('mobile_price_range_data.csv')
df.head()
battery_power	blue	clock_speed	dual_sim	fc	four_g	int_memory	m_dep	mobile_wt	n_cores	...	px_height	px_width	ram	sc_h	sc_w	talk_time	three_g	touch_screen	wifi	price_range
0	842	0	2.2	0	1	0	7	0.6	188	2	...	20	756	2549	9	7	19	0	0	1	1
1	1021	1	0.5	1	0	1	53	0.7	136	3	...	905	1988	2631	17	3	7	1	1	0	2
2	563	1	0.5	1	2	1	41	0.9	145	5	...	1263	1716	2603	11	2	9	1	1	0	2
3	615	1	2.5	0	0	0	10	0.8	131	6	...	1216	1786	2769	16	8	11	1	0	0	2
4	1821	1	1.2	0	13	1	44	0.6	141	2	...	1208	1212	1411	8	2	15	1	1	0	1
5 rows × 21 columns

df.shape
(2000, 21)
data preprocessing
df.isnull().sum()
battery_power    0
blue             0
clock_speed      0
dual_sim         0
fc               0
four_g           0
int_memory       0
m_dep            0
mobile_wt        0
n_cores          0
pc               0
px_height        0
px_width         0
ram              0
sc_h             0
sc_w             0
talk_time        0
three_g          0
touch_screen     0
wifi             0
price_range      0
dtype: int64
df.duplicated().sum()
0
df.dtypes
battery_power      int64
blue               int64
clock_speed      float64
dual_sim           int64
fc                 int64
four_g             int64
int_memory         int64
m_dep            float64
mobile_wt          int64
n_cores            int64
pc                 int64
px_height          int64
px_width           int64
ram                int64
sc_h               int64
sc_w               int64
talk_time          int64
three_g            int64
touch_screen       int64
wifi               int64
price_range        int64
dtype: object
df.head()
battery_power	blue	clock_speed	dual_sim	fc	four_g	int_memory	m_dep	mobile_wt	n_cores	...	px_height	px_width	ram	sc_h	sc_w	talk_time	three_g	touch_screen	wifi	price_range
0	842	0	2.2	0	1	0	7	0.6	188	2	...	20	756	2549	9	7	19	0	0	1	1
1	1021	1	0.5	1	0	1	53	0.7	136	3	...	905	1988	2631	17	3	7	1	1	0	2
2	563	1	0.5	1	2	1	41	0.9	145	5	...	1263	1716	2603	11	2	9	1	1	0	2
3	615	1	2.5	0	0	0	10	0.8	131	6	...	1216	1786	2769	16	8	11	1	0	0	2
4	1821	1	1.2	0	13	1	44	0.6	141	2	...	1208	1212	1411	8	2	15	1	1	0	1
5 rows × 21 columns

df['price_range'].value_counts()
1    500
2    500
3    500
0    500
Name: price_range, dtype: int64
#dependent and independent variable
x=df.drop('price_range', axis=1)
y=df['price_range']
print(type(x))
print(type(y))
print(x.shape)
print(y.shape)
<class 'pandas.core.frame.DataFrame'>
<class 'pandas.core.series.Series'>
(2000, 20)
(2000,)
x.head()
battery_power	blue	clock_speed	dual_sim	fc	four_g	int_memory	m_dep	mobile_wt	n_cores	pc	px_height	px_width	ram	sc_h	sc_w	talk_time	three_g	touch_screen	wifi
0	842	0	2.2	0	1	0	7	0.6	188	2	2	20	756	2549	9	7	19	0	0	1
1	1021	1	0.5	1	0	1	53	0.7	136	3	6	905	1988	2631	17	3	7	1	1	0
2	563	1	0.5	1	2	1	41	0.9	145	5	6	1263	1716	2603	11	2	9	1	1	0
3	615	1	2.5	0	0	0	10	0.8	131	6	9	1216	1786	2769	16	8	11	1	0	0
4	1821	1	1.2	0	13	1	44	0.6	141	2	14	1208	1212	1411	8	2	15	1	1	0
split the data into train and test
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=30)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
(1500, 20)
(500, 20)
(1500,)
(500,)
Model 1 (Logistic Regression)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix,classification_report
m1_log=LogisticRegression()
m1_log.fit(x_train,y_train)
LogisticRegression()
print('train score: ',m1_log.score(x_train,y_train))
print('Test score: ',m1_log.score(x_test,y_test))
train score:  0.6373333333333333
Test score:  0.662
ypred_m1_log=m1_log.predict(x_test)
print(ypred_m1_log)
[3 0 2 2 2 0 3 3 2 1 2 0 2 0 2 1 1 2 3 1 2 0 2 0 3 3 1 0 1 1 3 1 1 2 1 3 1
 3 2 3 1 0 2 0 3 2 0 1 3 3 2 0 1 1 0 2 2 1 2 0 0 2 1 1 1 0 0 1 2 0 2 0 2 3
 1 3 1 1 3 3 1 3 0 0 2 3 3 0 0 3 3 3 1 0 2 3 2 0 3 1 3 1 0 0 3 0 3 1 0 1 3
 3 0 1 3 3 0 0 1 1 0 3 2 3 2 1 3 3 1 0 2 2 3 0 0 0 0 0 3 3 1 2 3 2 1 2 1 2
 3 3 0 0 3 0 3 2 2 2 2 2 3 3 3 1 2 0 2 0 1 1 2 3 2 0 3 3 0 3 0 3 3 0 3 3 2
 1 1 3 1 1 1 2 1 2 0 0 1 3 2 0 1 3 3 0 3 1 0 2 1 1 1 0 3 3 3 3 1 1 3 3 2 3
 1 0 3 0 3 2 3 2 3 1 2 2 3 0 0 2 2 1 2 1 0 3 2 3 0 0 3 0 1 0 3 2 0 3 2 0 1
 0 0 0 0 1 3 3 3 2 1 1 1 1 2 3 2 2 3 1 2 3 3 3 0 2 0 2 3 3 2 2 1 2 3 3 3 0
 0 0 2 1 3 2 0 1 2 1 1 3 0 1 3 0 3 3 2 0 1 2 0 1 3 1 1 2 2 3 0 2 3 2 3 3 3
 3 3 2 2 3 1 3 0 1 0 2 1 3 3 2 1 3 0 2 3 3 3 0 0 2 0 0 0 2 3 3 0 0 3 2 1 2
 2 0 1 0 3 2 1 3 2 1 2 1 1 1 3 3 1 0 2 3 0 2 0 0 0 1 3 0 2 3 2 0 3 2 3 1 0
 0 2 3 2 0 2 2 0 3 3 3 1 2 0 1 1 1 3 1 2 1 1 3 0 3 0 1 3 3 3 2 3 1 0 3 1 0
 2 2 1 1 1 3 0 0 2 1 1 3 3 2 0 1 3 0 0 1 3 1 3 1 3 0 2 1 2 1 2 3 3 0 1 3 2
 2 0 0 2 1 2 3 3 3 1 3 1 1 3 2 0 2 1 1]
print(confusion_matrix(y_test,ypred_m1_log))
print(classification_report(y_test,ypred_m1_log))
[[ 99  19   2   0]
 [ 18  75  28   5]
 [  0  22  52  40]
 [  0   2  33 105]]
              precision    recall  f1-score   support

           0       0.85      0.82      0.84       120
           1       0.64      0.60      0.61       126
           2       0.45      0.46      0.45       114
           3       0.70      0.75      0.72       140

    accuracy                           0.66       500
   macro avg       0.66      0.66      0.66       500
weighted avg       0.66      0.66      0.66       500

Logistic Regression model have an accuracy of 66%
Model 2 (KNN_Algorithm)
from sklearn.neighbors import KNeighborsClassifier
m1_knn=KNeighborsClassifier(n_neighbors=11)
m1_knn.fit(x_train,y_train)
KNeighborsClassifier(n_neighbors=11)
print('train score: ',m1_knn.score(x_train,y_train))
print('Test score: ',m1_knn.score(x_test,y_test))
train score:  0.9493333333333334
Test score:  0.942
ypred_m1_knn=m1_knn.predict(x_test)
print(ypred_m1_knn)
[3 0 1 1 2 1 3 3 2 1 1 0 2 0 3 1 0 2 3 1 3 0 3 0 3 3 1 0 1 0 3 0 2 2 1 3 1
 3 2 3 2 1 2 0 3 3 1 0 3 2 2 0 3 2 0 2 3 1 1 0 0 3 1 0 1 0 0 1 3 0 1 0 2 3
 1 2 1 1 3 3 0 3 0 0 2 3 3 0 0 3 3 3 2 1 1 3 2 0 3 0 2 1 0 0 3 0 3 1 1 2 3
 1 0 1 2 3 0 1 2 1 1 3 2 2 2 1 3 2 1 0 3 2 2 0 0 1 0 1 3 3 1 2 3 2 1 1 0 3
 3 2 0 1 3 0 3 3 3 2 1 2 1 3 3 1 1 0 3 0 1 1 3 3 1 0 2 2 0 3 0 2 2 0 2 3 2
 1 2 3 1 1 1 0 1 3 0 0 1 3 2 1 0 3 3 0 3 1 0 2 1 2 1 1 3 3 3 3 0 0 3 3 3 2
 1 1 3 0 2 3 2 3 3 1 3 2 2 0 0 3 3 1 0 1 0 3 1 2 0 1 3 0 0 0 3 2 0 2 3 0 0
 0 0 1 0 1 2 3 3 2 2 1 1 1 2 3 1 3 2 3 2 3 1 2 0 2 1 2 2 3 2 1 1 1 3 2 3 0
 0 1 1 1 3 1 0 1 1 0 1 3 0 0 3 0 3 2 3 0 1 2 0 1 3 1 0 2 1 3 0 1 3 2 3 3 2
 3 3 2 3 2 2 2 0 2 0 1 0 3 3 2 1 2 0 2 2 3 2 0 0 2 0 0 0 3 2 2 0 0 3 3 1 2
 2 0 1 0 2 2 1 3 2 1 1 2 0 2 2 3 1 0 2 3 0 2 0 0 0 2 3 0 2 3 2 1 1 3 3 0 0
 0 3 3 2 1 3 1 0 2 3 3 1 3 0 1 2 1 3 1 1 1 0 2 0 3 0 1 2 3 2 1 3 2 0 3 2 0
 3 3 1 1 1 3 0 0 1 2 1 3 2 2 0 1 2 0 0 2 2 1 1 1 2 0 1 0 2 2 1 3 3 0 1 3 2
 2 0 1 2 1 2 2 3 3 0 3 2 1 2 2 0 2 1 1]
print(confusion_matrix(y_test,ypred_m1_knn))
print(classification_report(y_test,ypred_m1_knn))
[[118   2   0   0]
 [  3 119   4   0]
 [  0   4 105   5]
 [  0   0  11 129]]
              precision    recall  f1-score   support

           0       0.98      0.98      0.98       120
           1       0.95      0.94      0.95       126
           2       0.88      0.92      0.90       114
           3       0.96      0.92      0.94       140

    accuracy                           0.94       500
   macro avg       0.94      0.94      0.94       500
weighted avg       0.94      0.94      0.94       500

KNN model have an accuracy of 94%
Model 3 (a) (SVM Model - Linear Kernel)
from sklearn.svm import SVC
m1_svc=SVC(kernel='linear',C=10)
m1_svc.fit(x_train,y_train)
SVC(C=10, kernel='linear')
print('train score: ',m1_svc.score(x_train,y_train))
print('Test score: ',m1_svc.score(x_test,y_test))
train score:  0.9873333333333333
Test score:  0.978
ypred_m1_svc=m1_svc.predict(x_test)
print(ypred_m1_svc)
[3 0 1 1 2 1 3 3 3 1 1 0 1 0 3 1 0 2 3 1 3 0 2 0 3 3 1 0 1 0 3 0 2 2 1 3 1
 3 2 3 2 1 2 0 3 3 1 0 3 2 3 0 3 2 0 2 3 1 1 0 0 3 1 0 1 0 0 1 3 0 1 0 3 3
 1 2 1 1 3 3 0 3 0 0 3 3 2 0 0 3 3 3 1 1 1 3 2 0 3 0 2 1 0 0 3 0 3 1 0 2 3
 1 0 1 2 3 0 0 2 1 1 3 2 3 2 1 3 2 1 0 3 2 2 0 0 0 0 1 3 3 1 2 3 2 1 1 1 3
 3 2 0 1 3 0 3 3 3 2 1 2 1 3 3 1 1 0 3 0 1 1 3 3 1 0 2 3 0 3 0 2 2 0 2 3 2
 1 2 3 1 1 1 0 1 3 0 0 1 3 2 1 0 3 3 0 3 1 0 2 1 2 1 1 3 3 3 3 0 0 3 3 3 2
 1 1 3 0 2 3 2 3 3 1 3 2 2 0 0 3 3 1 0 1 0 3 1 3 0 1 3 0 0 0 3 1 0 2 3 0 0
 0 0 1 0 1 2 3 3 2 2 1 1 1 2 3 1 3 2 3 2 3 1 2 0 2 1 2 2 3 2 1 1 1 3 3 3 0
 0 1 1 1 3 1 0 1 1 0 1 3 0 1 3 0 2 3 3 0 1 2 0 1 3 1 0 2 1 3 0 1 3 2 3 3 3
 3 3 2 3 2 2 2 0 2 0 1 1 3 3 2 1 2 0 2 2 3 2 0 0 2 0 0 0 3 3 3 0 0 3 3 1 2
 2 0 2 0 2 2 1 3 2 1 1 1 0 2 2 3 1 0 2 3 0 2 0 0 0 2 3 0 2 3 2 1 1 3 3 0 0
 0 3 3 2 1 2 1 0 2 3 3 1 3 0 1 2 1 3 1 1 1 0 3 0 3 0 1 2 3 3 1 3 2 0 3 2 0
 3 3 1 1 1 3 0 0 1 2 1 3 2 2 0 1 2 0 0 2 2 1 1 1 2 0 1 0 2 2 1 3 3 0 1 3 2
 2 0 1 2 1 2 2 3 3 0 3 1 1 2 2 0 2 1 1]
print(confusion_matrix(y_test,ypred_m1_svc))
print(classification_report(y_test,ypred_m1_svc))
[[120   0   0   0]
 [  1 125   0   0]
 [  0   4 105   5]
 [  0   0   1 139]]
              precision    recall  f1-score   support

           0       0.99      1.00      1.00       120
           1       0.97      0.99      0.98       126
           2       0.99      0.92      0.95       114
           3       0.97      0.99      0.98       140

    accuracy                           0.98       500
   macro avg       0.98      0.98      0.98       500
weighted avg       0.98      0.98      0.98       500

SVM model (Linear Kernel) have an accuracy of 98%
Model 3 (b) (SVM Model - RBF Kernel)
m1_svc2=SVC(kernel='rbf',C=10)
m1_svc2.fit(x_train,y_train)
SVC(C=10)
print('train score: ',m1_svc2.score(x_train,y_train))
print('Test score: ',m1_svc2.score(x_test,y_test))
train score:  0.9653333333333334
Test score:  0.964
ypred_m1_svc2=m1_svc.predict(x_test)
print(ypred_m1_svc2)
[3 0 1 1 2 1 3 3 3 1 1 0 1 0 3 1 0 2 3 1 3 0 2 0 3 3 1 0 1 0 3 0 2 2 1 3 1
 3 2 3 2 1 2 0 3 3 1 0 3 2 3 0 3 2 0 2 3 1 1 0 0 3 1 0 1 0 0 1 3 0 1 0 3 3
 1 2 1 1 3 3 0 3 0 0 3 3 2 0 0 3 3 3 1 1 1 3 2 0 3 0 2 1 0 0 3 0 3 1 0 2 3
 1 0 1 2 3 0 0 2 1 1 3 2 3 2 1 3 2 1 0 3 2 2 0 0 0 0 1 3 3 1 2 3 2 1 1 1 3
 3 2 0 1 3 0 3 3 3 2 1 2 1 3 3 1 1 0 3 0 1 1 3 3 1 0 2 3 0 3 0 2 2 0 2 3 2
 1 2 3 1 1 1 0 1 3 0 0 1 3 2 1 0 3 3 0 3 1 0 2 1 2 1 1 3 3 3 3 0 0 3 3 3 2
 1 1 3 0 2 3 2 3 3 1 3 2 2 0 0 3 3 1 0 1 0 3 1 3 0 1 3 0 0 0 3 1 0 2 3 0 0
 0 0 1 0 1 2 3 3 2 2 1 1 1 2 3 1 3 2 3 2 3 1 2 0 2 1 2 2 3 2 1 1 1 3 3 3 0
 0 1 1 1 3 1 0 1 1 0 1 3 0 1 3 0 2 3 3 0 1 2 0 1 3 1 0 2 1 3 0 1 3 2 3 3 3
 3 3 2 3 2 2 2 0 2 0 1 1 3 3 2 1 2 0 2 2 3 2 0 0 2 0 0 0 3 3 3 0 0 3 3 1 2
 2 0 2 0 2 2 1 3 2 1 1 1 0 2 2 3 1 0 2 3 0 2 0 0 0 2 3 0 2 3 2 1 1 3 3 0 0
 0 3 3 2 1 2 1 0 2 3 3 1 3 0 1 2 1 3 1 1 1 0 3 0 3 0 1 2 3 3 1 3 2 0 3 2 0
 3 3 1 1 1 3 0 0 1 2 1 3 2 2 0 1 2 0 0 2 2 1 1 1 2 0 1 0 2 2 1 3 3 0 1 3 2
 2 0 1 2 1 2 2 3 3 0 3 1 1 2 2 0 2 1 1]
print(confusion_matrix(y_test,ypred_m1_svc2))
print(classification_report(y_test,ypred_m1_svc2))
[[120   0   0   0]
 [  1 125   0   0]
 [  0   4 105   5]
 [  0   0   1 139]]
              precision    recall  f1-score   support

           0       0.99      1.00      1.00       120
           1       0.97      0.99      0.98       126
           2       0.99      0.92      0.95       114
           3       0.97      0.99      0.98       140

    accuracy                           0.98       500
   macro avg       0.98      0.98      0.98       500
weighted avg       0.98      0.98      0.98       500

SVM model(rbf kernel) have an accuracy of 98%
Model 4(Decision Tree classifier Model)
from sklearn.tree import DecisionTreeClassifier
m1_dtc=DecisionTreeClassifier()
m1_dtc.fit(x_train,y_train)
DecisionTreeClassifier()
print('train score: ',m1_dtc.score(x_train,y_train))
print('Test score: ',m1_dtc.score(x_test,y_test))
train score:  1.0
Test score:  0.834
ypred_m1_dtc=m1_dtc.predict(x_test)
print(ypred_m1_dtc)
[3 0 1 1 2 1 3 3 2 1 2 0 1 0 2 1 0 2 3 1 3 0 2 0 3 2 1 1 0 0 3 0 1 2 1 3 2
 3 2 3 1 1 2 0 3 3 1 0 3 3 3 0 3 2 0 2 3 1 2 0 0 3 1 0 1 0 0 1 3 0 1 0 2 3
 1 2 2 1 3 3 0 3 0 1 3 3 3 0 0 3 3 3 2 1 1 3 2 0 3 0 3 1 0 0 3 0 3 2 0 2 2
 1 0 1 2 3 0 0 2 1 1 3 2 3 2 1 3 2 1 0 3 2 2 0 0 1 0 1 3 3 2 2 2 2 1 1 0 2
 3 3 0 1 3 0 3 3 3 2 1 2 2 3 3 1 2 0 2 0 1 2 3 3 1 0 3 2 0 2 0 2 1 0 2 2 2
 1 2 3 1 1 1 1 1 3 0 0 1 3 2 2 1 3 3 0 3 0 0 2 1 1 1 1 3 3 3 3 0 1 3 3 3 2
 1 1 3 0 3 3 2 3 3 1 3 2 2 0 0 3 3 2 0 2 0 3 1 3 0 1 3 0 1 0 3 2 0 2 3 0 1
 0 0 1 0 1 2 3 3 2 2 1 1 1 2 3 1 3 2 2 2 3 1 2 0 2 0 2 2 3 2 1 1 2 2 2 3 0
 0 0 1 1 3 1 0 1 1 1 2 3 0 1 3 0 2 2 2 0 1 2 0 1 3 1 0 2 1 3 0 1 3 2 3 3 3
 3 3 2 2 2 1 2 0 1 0 1 0 3 3 2 1 2 0 2 2 3 2 0 0 2 0 0 0 3 2 2 0 0 3 3 1 3
 2 0 1 0 1 3 1 3 2 1 1 2 0 2 2 3 2 0 2 2 0 1 0 0 0 2 3 0 2 3 2 0 1 3 3 0 0
 0 3 3 2 1 3 1 0 2 3 3 1 3 0 2 2 2 2 1 1 1 0 2 0 3 0 1 2 3 3 1 3 2 0 3 1 0
 3 3 1 1 1 3 0 0 1 2 1 3 3 2 0 1 2 0 0 2 2 1 2 1 2 0 1 0 2 2 1 3 3 0 1 3 3
 2 0 1 2 1 2 2 3 3 0 3 2 2 2 2 0 3 2 1]
print(confusion_matrix(y_test,ypred_m1_dtc))
print(classification_report(y_test,ypred_m1_dtc))
[[111   9   0   0]
 [  8  96  22   0]
 [  0  12  89  13]
 [  0   0  19 121]]
              precision    recall  f1-score   support

           0       0.93      0.93      0.93       120
           1       0.82      0.76      0.79       126
           2       0.68      0.78      0.73       114
           3       0.90      0.86      0.88       140

    accuracy                           0.83       500
   macro avg       0.84      0.83      0.83       500
weighted avg       0.84      0.83      0.84       500

Decision Tree Classifier Model have an accuracy of 83%
Model 4(Random Forest Classifier Model)
from sklearn.ensemble import RandomForestClassifier
m1_rfc=RandomForestClassifier(n_estimators=100)
m1_rfc.fit(x_train,y_train)
RandomForestClassifier()
print('train score: ',m1_rfc.score(x_train,y_train))
print('Test score: ',m1_rfc.score(x_test,y_test))
train score:  1.0
Test score:  0.86
ypred_m1_rfc=m1_rfc.predict(x_test)
print(ypred_m1_rfc)
[2 0 1 1 2 1 3 3 2 1 1 0 2 0 2 1 0 2 3 1 3 0 3 0 3 3 1 0 0 0 3 0 2 2 1 3 1
 2 2 2 1 1 2 0 3 3 1 0 3 3 2 0 3 1 0 2 3 1 1 0 0 3 1 0 1 0 0 1 3 0 1 0 2 3
 1 2 1 1 3 3 0 3 0 0 2 3 3 0 0 3 3 3 2 1 1 3 2 0 3 0 2 1 0 0 3 0 3 1 1 2 3
 2 0 1 2 3 0 1 2 1 1 3 2 3 2 1 3 2 1 1 3 2 2 0 0 1 0 1 3 3 1 2 2 2 1 1 0 3
 3 2 0 1 3 0 3 3 3 2 1 2 2 3 3 1 1 0 2 0 1 1 3 3 1 0 2 2 0 3 0 3 2 0 2 3 2
 2 2 3 1 1 1 1 1 3 0 0 0 3 2 1 0 3 3 0 3 1 0 2 1 1 1 0 3 3 3 3 0 0 3 3 3 2
 1 1 3 0 2 3 2 3 3 1 3 2 2 0 0 3 3 1 0 1 0 3 1 2 0 1 3 0 0 0 3 2 0 2 3 0 0
 0 0 1 0 1 2 3 3 2 2 1 1 1 2 3 1 3 1 3 2 3 2 2 0 2 0 2 2 3 2 2 1 2 3 3 3 0
 0 1 2 1 3 1 1 1 1 1 1 3 0 0 3 0 3 2 3 0 1 2 0 1 3 1 0 2 2 3 0 1 3 1 3 3 3
 3 3 2 3 2 1 2 0 1 0 1 0 3 3 2 1 2 0 2 2 3 2 0 0 2 0 0 0 2 2 3 0 0 3 3 1 3
 2 0 1 0 2 2 1 3 2 1 1 1 0 2 2 3 1 0 2 3 0 2 0 0 0 2 3 0 2 3 2 1 2 3 3 0 0
 0 3 3 2 1 3 1 0 2 3 3 1 3 0 1 2 1 3 1 1 1 0 2 0 3 0 1 2 3 3 1 3 2 0 3 1 0
 3 3 1 1 1 3 0 0 1 2 1 3 2 2 0 1 2 0 0 1 2 1 2 2 2 0 1 0 2 2 1 3 3 0 1 3 2
 2 0 1 2 1 2 1 3 3 0 3 1 2 3 2 0 3 1 1]
print(confusion_matrix(y_test,ypred_m1_rfc))
print(classification_report(y_test,ypred_m1_rfc))
[[114   6   0   0]
 [  7 104  15   0]
 [  0  15  87  12]
 [  0   0  15 125]]
              precision    recall  f1-score   support

           0       0.94      0.95      0.95       120
           1       0.83      0.83      0.83       126
           2       0.74      0.76      0.75       114
           3       0.91      0.89      0.90       140

    accuracy                           0.86       500
   macro avg       0.86      0.86      0.86       500
weighted avg       0.86      0.86      0.86       500

Random forest classifier have an accuracy of 86%
Model with the best accuracy is - Support Vector Machine(SVM) Model with an accuracy of 98%